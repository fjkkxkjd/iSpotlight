# iSpotlight: An Attention-Aware Immersive Learning System

iSpotlight is an intelligent immersive learning system designed to support fine-grained attention analysis in virtual environments. This project integrates multiple modules for video and audio processing, eye-tracking analysis, and cross-modality knowledge association using large language models (LLMs) like GPT-4.

## Introduction

iSpotlight enables immersive learning by aligning visual elements from lecture slides with corresponding audio content, providing real-time feedback on learner engagement through eye-tracking data. The system automates labor-intensive tasks such as extracting elements of interest (EoIs) from slides and linking them to knowledge points discussed by the instructor, helping to track and augment the learner's attention.

## Project Structure


```
# Dataset
├── user_study_eyetracking_attention/   # Contains study-related data for eyetracking
├── video_data/                         # Raw lecture videos
├── video_result_data/                  # Output results from lecture videos processing
│   ├── {video_name}_data/
│   │   ├── json_files/                 # Knowledge points data for key frames
│   │   │   ├── [timestamp of key frame].json  # JSON-structured knowledge points generated by GPT for the key frame
│   │   │   ├── [timestamp of key frame].txt   # Text version of the knowledge points generated by GPT for the key frame
│   │   ├── object_frames/              # The output of Visual EOIs detection 
│   │   │   ├── [timestamp of key frame]_[index].jpg
│   │   ├── original_frames/            # Key fames
│   │   │   ├── [timestamp of key frame].jpg
│   │   ├── result_json_files/          # The result of Transcript-to-Knowledge Matching
│   │   │   ├── [timestamp of key frame].json  # JSON-structured result
│   │   │   ├── [timestamp of key frame].txt   # txt-structured result
│   │   ├── voice_json_file/                   # transcription result with timestamp
│   │   │   ├── [timestamp of key frame].json  # JSON-structured transcription result for key frames
│   │   ├── {video_name}.tsv            # Transcription result
│   │   ├── merge.txt                   # The reuslt of EOIs extract after region fusion
│   │   ├── final_match_merge.txt       # Contains the results of merging regions with the corresponding knowledge points
│   │   ├── recognized_text.txt         # The reuslt of EOIs extract before region fusion
│   │   ├── timestamps.txt              # The timestamps of key frames
│   │   ├── voice_match_final.txt       # Contains the knowledge points being explained during each time segment
│   │   ├── merged_results.json         # Contains the regions being explained during each time segment

├── weights/                            # Model weights

├── transition_dataset/                 # Data related to detecting transition
│   ├── train/                 
│   │   ├── 2/                           # Frames collected from each video in the training set   
│   │   │   ├── 1.jpg
│   │   │   ├── 2.jpg          
│   ├── test/
│   │   ├── high_frames/                 # Frames of high frame rate videos
│   │   │   ├── 1/
│   │   │   │   ├── 1.jpg
│   │   │   │   ├── 2.jpg      
│   │   ├── low_frames/                  # Frames of low frame rate videos

├── visual_elements_detection_data/      # Data related to detecting visual elements(YOLOv9)
│   ├── dataset1/                        # YOLOv9 dataset (5-fold cross-validation), more details referenced from the YOLOv9
│   ├── dataset2/ 
│   ├── dataset3/ 
│   ├── dataset4/ 
│   ├── dataset5/  

├── eye_tracking_data/                   # Eye-tracking data collected from VR devices
│   ├── user_1/
│   │   ├── EyeTracking_202408122131.txt # Eye-tracking data collected from VR devices



# Visual EoIs extractor model YOLOv9(For specific details, please refer to the YOLOv9 documentation)
├── data                                # YAML (YAML Ain't Markup Language) configuration file
├── train_video.py                      # Train algorithm
├── models                              # Models
├── val_dual.py                         # Val algorithm
├── detect_dual.py                      # Dual detection algorithm
├── export.py                           # Export function
├── utils/                              # Utility scripts

# Pipeline
├── main.py                             # Main pipeline for processing
├── gpt.py                              # GPT processing scripts (e.g., keypoints)
├── kp_match_merge.py                   # Keypoint match with EOIs and merge module
├── overlap.py                          # Overlap detection algorithm

Cross-modality Knowledge Points Matching
├── voice_match.py                      # Matching voice data with knowledge points
├── voice.py                            # Transcript-to-Knowledge Matching
├── voice_to_txt.py                     # Speech-to-transcription Conversion

Attention visualization analyzer
├── gaze_mode.py                        # Analyzing gaze patterns to evaluate attention.
├── kp_video_visual.py                  # Annotate video with knowledge point areas
├── eyetracking_visual.py               # Script for visualizing eye-tracking data on lecture videos

└── requirements.txt                    # List of dependencies
```

## Setup Instructions

To set up and run the iSpotlight project:

1. **Clone the repository**:
   ```bash
   git clone https://github.com/your-repo/ispotlight.git
   cd ispotlight
   ```

2. **Install dependencies**:
   Ensure you are in a virtual environment and run the following command:
   ```bash
   pip install -r requirements.txt
   ```

3. **Download necessary weights**:
   Make sure the pre-trained weights are available in the `weights/` directory, such as `yolo_best.pt` and `large-v3.pt`.

4. **Prepare the data**:
   Place the input video files in the `video_data/` folder and ensure the proper structure of the directories (`json_files/`, `voice_data/`, etc.) for outputs.

## Dependencies

A complete list of Python libraries and versions can be found in the `requirements.txt` file. 

## Note

Before running the `iSpotlight` project, make sure to set your OpenAI API Key in the `gpt.py` file. Find the following line of code and replace `api_key` with your actual API key:
api_key = 'your_openai_api_key_here'

The project uses models and methods such as Whisper, YOLOv9, and others. If you encounter any issues while using these models, please refer to their official documentation for guidance.
## Usage

1. **Run the Main Pipeline**:
   To start processing a video and aligning the detected elements with knowledge points, run:
   ```bash
   python main.py
   ```

   The main steps include:
   - Detecting visual elements from the video
   - Recognizing and aligning textual elements
   - Processing audio to text using Whisper
   - Merging key points with lecture content

2. **YOLOv9 Training**:
   To train the YOLOv9 model, run `train_video.py`. For validation and testing, use `val_dual.py` and `detect_dual.py`. For parameter settings, please refer to the YOLOv9 documentation.

3. **Training and Testing ViT_MLFF**:
   To train and test the ViT_MLFF model, run `vit_mlff.py`. For testing the complete method, run `intensity_vit_mlff.py`.

4. **Custom Functions**:
   You can run individual modules if needed:
   - **OCR and formula recognition**: `recognize_formula(frame)`
   - **Video similarity detection**: `predict_similarity(model, image1, image2)`

5. **Visual Output**:
   Processed frames and videos with bounding boxes around detected elements and OCR results are saved in `video_result_data/`.

6. **Eye-Tracking Data Analysis**:
   The script `gaze_mode.py` can be used to batch process users' eye-tracking data(eye_tracking_data) by setting the user data folder, allowing for an analysis of their attention levels during class through gaze pattern analysis.

## Limitations

1. **Instructor Presence**: The system may struggle when the instructor's presence (e.g., standing in front of slides) obstructs key slide content.
2. **Complex Slide Layouts**: Overlapping text and visual elements on slides may lead to incorrect region detections.
3. **Audio-Slide Divergence**: When the instructor deviates from the content on the slide, the system may face challenges aligning audio with slide content accurately.

These limitations suggest opportunities for future work in improving alignment algorithms and handling more complex lecture formats.

## Project Resources

Our project code is available on GitHub: [\[GitHub link\]](https://github.com/fjkkxkjd/iSpotlight).
The Unity project code for pico4 pro will be made available in our GitHub repository. Please check the repository for updates and access to the code.

Our dataset can be accessed via OneDrive: [\[OneDrive link\]](https://1drv.ms/f/c/ab8db1e1fd9a0c0c/EiPCQo9g0PpPi1UTGIXbvooBynY_i3nLDSobJWRmfVEZLA?e=xFd3FY).
Once downloaded, place the weights in the `weights/` directory in the project folder. Ensure that the necessary pre-trained models (e.g., `yolo_best.pt`, `large-v3.pt`) are properly located for the system to function correctly.
If any issues arise, we will update the latest progress on these platforms.
Additionally, the resource package for our Unity-based VR project Education_2 can also be accessed via OneDrive. In this VR scene, it is possible to simulate classroom lessons and collect eye-tracking data, head posture data, and facial expression data during video playback. The device used is Pico 4pro, and the Unity version is 2022.3.24f1.
